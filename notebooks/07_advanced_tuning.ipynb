{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hyperparam Tuning for ADA ===\n",
      "  ADA/Close_t+1 => Tuning RandomForest ...\n",
      "  ADA/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.026). Saved => ../models\\ADA_Close_t_plus_1_RF.pkl\n",
      "  ADA/Close_t+7 => Tuning RandomForest ...\n",
      "  ADA/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=0.075). Saved => ../models\\ADA_Close_t_plus_7_XGB.pkl\n",
      "  ADA/Close_t+30 => Tuning RandomForest ...\n",
      "  ADA/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.194). Saved => ../models\\ADA_Close_t_plus_30_RF.pkl\n",
      "  ADA/Close_t+90 => Tuning RandomForest ...\n",
      "  ADA/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=0.268). Saved => ../models\\ADA_Close_t_plus_90_XGB.pkl\n",
      "\n",
      "=== Hyperparam Tuning for AVAX ===\n",
      "  AVAX/Close_t+1 => Tuning RandomForest ...\n",
      "  AVAX/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=2.339). Saved => ../models\\AVAX_Close_t_plus_1_RF.pkl\n",
      "  AVAX/Close_t+7 => Tuning RandomForest ...\n",
      "  AVAX/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=5.922). Saved => ../models\\AVAX_Close_t_plus_7_RF.pkl\n",
      "  AVAX/Close_t+30 => Tuning RandomForest ...\n",
      "  AVAX/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=10.026). Saved => ../models\\AVAX_Close_t_plus_30_XGB.pkl\n",
      "  AVAX/Close_t+90 => Tuning RandomForest ...\n",
      "  AVAX/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=19.605). Saved => ../models\\AVAX_Close_t_plus_90_RF.pkl\n",
      "\n",
      "=== Hyperparam Tuning for BCH ===\n",
      "  BCH/Close_t+1 => Tuning RandomForest ...\n",
      "  BCH/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=20.674). Saved => ../models\\BCH_Close_t_plus_1_RF.pkl\n",
      "  BCH/Close_t+7 => Tuning RandomForest ...\n",
      "  BCH/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=49.001). Saved => ../models\\BCH_Close_t_plus_7_RF.pkl\n",
      "  BCH/Close_t+30 => Tuning RandomForest ...\n",
      "  BCH/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=86.710). Saved => ../models\\BCH_Close_t_plus_30_XGB.pkl\n",
      "  BCH/Close_t+90 => Tuning RandomForest ...\n",
      "  BCH/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=198.658). Saved => ../models\\BCH_Close_t_plus_90_RF.pkl\n",
      "\n",
      "=== Hyperparam Tuning for BNB ===\n",
      "  BNB/Close_t+1 => Tuning RandomForest ...\n",
      "  BNB/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=28.529). Saved => ../models\\BNB_Close_t_plus_1_RF.pkl\n",
      "  BNB/Close_t+7 => Tuning RandomForest ...\n",
      "  BNB/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=60.139). Saved => ../models\\BNB_Close_t_plus_7_RF.pkl\n",
      "  BNB/Close_t+30 => Tuning RandomForest ...\n",
      "  BNB/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=134.312). Saved => ../models\\BNB_Close_t_plus_30_XGB.pkl\n",
      "  BNB/Close_t+90 => Tuning RandomForest ...\n",
      "  BNB/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=199.267). Saved => ../models\\BNB_Close_t_plus_90_RF.pkl\n",
      "\n",
      "=== Hyperparam Tuning for BTC ===\n",
      "  BTC/Close_t+1 => Tuning RandomForest ...\n",
      "  BTC/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=7387.171). Saved => ../models\\BTC_Close_t_plus_1_RF.pkl\n",
      "  BTC/Close_t+7 => Tuning RandomForest ...\n",
      "  BTC/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=10812.362). Saved => ../models\\BTC_Close_t_plus_7_XGB.pkl\n",
      "  BTC/Close_t+30 => Tuning RandomForest ...\n",
      "  BTC/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=18506.072). Saved => ../models\\BTC_Close_t_plus_30_RF.pkl\n",
      "  BTC/Close_t+90 => Tuning RandomForest ...\n",
      "  BTC/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=31299.211). Saved => ../models\\BTC_Close_t_plus_90_RF.pkl\n",
      "\n",
      "=== Hyperparam Tuning for DOGE ===\n",
      "  DOGE/Close_t+1 => Tuning RandomForest ...\n",
      "  DOGE/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.013). Saved => ../models\\DOGE_Close_t_plus_1_RF.pkl\n",
      "  DOGE/Close_t+7 => Tuning RandomForest ...\n",
      "  DOGE/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=0.030). Saved => ../models\\DOGE_Close_t_plus_7_XGB.pkl\n",
      "  DOGE/Close_t+30 => Tuning RandomForest ...\n",
      "  DOGE/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=0.072). Saved => ../models\\DOGE_Close_t_plus_30_XGB.pkl\n",
      "  DOGE/Close_t+90 => Tuning RandomForest ...\n",
      "  DOGE/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=0.106). Saved => ../models\\DOGE_Close_t_plus_90_XGB.pkl\n",
      "\n",
      "=== Hyperparam Tuning for DOT ===\n",
      "  DOT/Close_t+1 => Tuning RandomForest ...\n",
      "  DOT/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.368). Saved => ../models\\DOT_Close_t_plus_1_RF.pkl\n",
      "  DOT/Close_t+7 => Tuning RandomForest ...\n",
      "  DOT/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=1.377). Saved => ../models\\DOT_Close_t_plus_7_RF.pkl\n",
      "  DOT/Close_t+30 => Tuning RandomForest ...\n",
      "  DOT/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=3.634). Saved => ../models\\DOT_Close_t_plus_30_XGB.pkl\n",
      "  DOT/Close_t+90 => Tuning RandomForest ...\n",
      "  DOT/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=7.980). Saved => ../models\\DOT_Close_t_plus_90_XGB.pkl\n",
      "\n",
      "=== Hyperparam Tuning for ETH ===\n",
      "  ETH/Close_t+1 => Tuning RandomForest ...\n",
      "  ETH/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=103.643). Saved => ../models\\ETH_Close_t_plus_1_RF.pkl\n",
      "  ETH/Close_t+7 => Tuning RandomForest ...\n",
      "  ETH/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=269.167). Saved => ../models\\ETH_Close_t_plus_7_RF.pkl\n",
      "  ETH/Close_t+30 => Tuning RandomForest ...\n",
      "  ETH/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=525.516). Saved => ../models\\ETH_Close_t_plus_30_XGB.pkl\n",
      "  ETH/Close_t+90 => Tuning RandomForest ...\n",
      "  ETH/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=750.367). Saved => ../models\\ETH_Close_t_plus_90_XGB.pkl\n",
      "\n",
      "=== Hyperparam Tuning for LEO ===\n",
      "  LEO/Close_t+1 => Tuning RandomForest ...\n",
      "  LEO/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.946). Saved => ../models\\LEO_Close_t_plus_1_RF.pkl\n",
      "  LEO/Close_t+7 => Tuning RandomForest ...\n",
      "  LEO/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=1.165). Saved => ../models\\LEO_Close_t_plus_7_RF.pkl\n",
      "  LEO/Close_t+30 => Tuning RandomForest ...\n",
      "  LEO/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=1.693). Saved => ../models\\LEO_Close_t_plus_30_RF.pkl\n",
      "  LEO/Close_t+90 => Tuning RandomForest ...\n",
      "  LEO/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=3.040). Saved => ../models\\LEO_Close_t_plus_90_RF.pkl\n",
      "\n",
      "=== Hyperparam Tuning for LINK ===\n",
      "  LINK/Close_t+1 => Tuning RandomForest ...\n",
      "  LINK/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.838). Saved => ../models\\LINK_Close_t_plus_1_RF.pkl\n",
      "  LINK/Close_t+7 => Tuning RandomForest ...\n",
      "  LINK/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=2.005). Saved => ../models\\LINK_Close_t_plus_7_XGB.pkl\n",
      "  LINK/Close_t+30 => Tuning RandomForest ...\n",
      "  LINK/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=4.616). Saved => ../models\\LINK_Close_t_plus_30_XGB.pkl\n",
      "  LINK/Close_t+90 => Tuning RandomForest ...\n",
      "  LINK/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=7.702). Saved => ../models\\LINK_Close_t_plus_90_RF.pkl\n",
      "\n",
      "=== Hyperparam Tuning for LTC ===\n",
      "  LTC/Close_t+1 => Tuning RandomForest ...\n",
      "  LTC/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=3.607). Saved => ../models\\LTC_Close_t_plus_1_RF.pkl\n",
      "  LTC/Close_t+7 => Tuning RandomForest ...\n",
      "  LTC/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=10.385). Saved => ../models\\LTC_Close_t_plus_7_RF.pkl\n",
      "  LTC/Close_t+30 => Tuning RandomForest ...\n",
      "  LTC/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=20.509). Saved => ../models\\LTC_Close_t_plus_30_XGB.pkl\n",
      "  LTC/Close_t+90 => Tuning RandomForest ...\n",
      "  LTC/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=20.134). Saved => ../models\\LTC_Close_t_plus_90_RF.pkl\n",
      "\n",
      "=== Hyperparam Tuning for MATIC ===\n",
      "  MATIC/Close_t+1 => Tuning RandomForest ...\n",
      "  MATIC/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=0.039). Saved => ../models\\MATIC_Close_t_plus_1_XGB.pkl\n",
      "  MATIC/Close_t+7 => Tuning RandomForest ...\n",
      "  MATIC/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=0.110). Saved => ../models\\MATIC_Close_t_plus_7_XGB.pkl\n",
      "  MATIC/Close_t+30 => Tuning RandomForest ...\n",
      "  MATIC/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.309). Saved => ../models\\MATIC_Close_t_plus_30_RF.pkl\n",
      "  MATIC/Close_t+90 => Tuning RandomForest ...\n",
      "  MATIC/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=0.369). Saved => ../models\\MATIC_Close_t_plus_90_XGB.pkl\n",
      "\n",
      "=== Hyperparam Tuning for NEAR ===\n",
      "  NEAR/Close_t+1 => Tuning RandomForest ...\n",
      "  NEAR/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.418). Saved => ../models\\NEAR_Close_t_plus_1_RF.pkl\n",
      "  NEAR/Close_t+7 => Tuning RandomForest ...\n",
      "  NEAR/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=1.221). Saved => ../models\\NEAR_Close_t_plus_7_XGB.pkl\n",
      "  NEAR/Close_t+30 => Tuning RandomForest ...\n",
      "  NEAR/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=2.326). Saved => ../models\\NEAR_Close_t_plus_30_XGB.pkl\n",
      "  NEAR/Close_t+90 => Tuning RandomForest ...\n",
      "  NEAR/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=3.677). Saved => ../models\\NEAR_Close_t_plus_90_XGB.pkl\n",
      "\n",
      "=== Hyperparam Tuning for SHIB ===\n",
      "  SHIB/Close_t+1 => Tuning RandomForest ...\n",
      "  SHIB/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.000). Saved => ../models\\SHIB_Close_t_plus_1_RF.pkl\n",
      "  SHIB/Close_t+7 => Tuning RandomForest ...\n",
      "  SHIB/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.000). Saved => ../models\\SHIB_Close_t_plus_7_RF.pkl\n",
      "  SHIB/Close_t+30 => Tuning RandomForest ...\n",
      "  SHIB/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.000). Saved => ../models\\SHIB_Close_t_plus_30_RF.pkl\n",
      "  SHIB/Close_t+90 => Tuning RandomForest ...\n",
      "  SHIB/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=0.000). Saved => ../models\\SHIB_Close_t_plus_90_XGB.pkl\n",
      "\n",
      "=== Hyperparam Tuning for SOL ===\n",
      "  SOL/Close_t+1 => Tuning RandomForest ...\n",
      "  SOL/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=8.530). Saved => ../models\\SOL_Close_t_plus_1_RF.pkl\n",
      "  SOL/Close_t+7 => Tuning RandomForest ...\n",
      "  SOL/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=23.289). Saved => ../models\\SOL_Close_t_plus_7_RF.pkl\n",
      "  SOL/Close_t+30 => Tuning RandomForest ...\n",
      "  SOL/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=47.992). Saved => ../models\\SOL_Close_t_plus_30_RF.pkl\n",
      "  SOL/Close_t+90 => Tuning RandomForest ...\n",
      "  SOL/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=69.968). Saved => ../models\\SOL_Close_t_plus_90_RF.pkl\n",
      "\n",
      "=== Hyperparam Tuning for TON ===\n",
      "  TON/Close_t+1 => Tuning RandomForest ...\n",
      "  TON/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.607). Saved => ../models\\TON_Close_t_plus_1_RF.pkl\n",
      "  TON/Close_t+7 => Tuning RandomForest ...\n",
      "  TON/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=1.060). Saved => ../models\\TON_Close_t_plus_7_RF.pkl\n",
      "  TON/Close_t+30 => Tuning RandomForest ...\n",
      "  TON/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.850). Saved => ../models\\TON_Close_t_plus_30_RF.pkl\n",
      "  TON/Close_t+90 => Tuning RandomForest ...\n",
      "  TON/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=1.118). Saved => ../models\\TON_Close_t_plus_90_XGB.pkl\n",
      "\n",
      "=== Hyperparam Tuning for TRX ===\n",
      "  TRX/Close_t+1 => Tuning RandomForest ...\n",
      "  TRX/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.029). Saved => ../models\\TRX_Close_t_plus_1_RF.pkl\n",
      "  TRX/Close_t+7 => Tuning RandomForest ...\n",
      "  TRX/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.048). Saved => ../models\\TRX_Close_t_plus_7_RF.pkl\n",
      "  TRX/Close_t+30 => Tuning RandomForest ...\n",
      "  TRX/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.077). Saved => ../models\\TRX_Close_t_plus_30_RF.pkl\n",
      "  TRX/Close_t+90 => Tuning RandomForest ...\n",
      "  TRX/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.103). Saved => ../models\\TRX_Close_t_plus_90_RF.pkl\n",
      "\n",
      "=== Hyperparam Tuning for UNI ===\n",
      "  UNI/Close_t+1 => Tuning RandomForest ...\n",
      "  UNI/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.791). Saved => ../models\\UNI_Close_t_plus_1_RF.pkl\n",
      "  UNI/Close_t+7 => Tuning RandomForest ...\n",
      "  UNI/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=2.062). Saved => ../models\\UNI_Close_t_plus_7_RF.pkl\n",
      "  UNI/Close_t+30 => Tuning RandomForest ...\n",
      "  UNI/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=3.823). Saved => ../models\\UNI_Close_t_plus_30_XGB.pkl\n",
      "  UNI/Close_t+90 => Tuning RandomForest ...\n",
      "  UNI/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=9.310). Saved => ../models\\UNI_Close_t_plus_90_XGB.pkl\n",
      "\n",
      "=== Hyperparam Tuning for XRP ===\n",
      "  XRP/Close_t+1 => Tuning RandomForest ...\n",
      "  XRP/Close_t+1 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.060). Saved => ../models\\XRP_Close_t_plus_1_RF.pkl\n",
      "  XRP/Close_t+7 => Tuning RandomForest ...\n",
      "  XRP/Close_t+7 => Tuning XGBoost ...\n",
      "    => Best: XGB (RMSE=0.179). Saved => ../models\\XRP_Close_t_plus_7_XGB.pkl\n",
      "  XRP/Close_t+30 => Tuning RandomForest ...\n",
      "  XRP/Close_t+30 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.442). Saved => ../models\\XRP_Close_t_plus_30_RF.pkl\n",
      "  XRP/Close_t+90 => Tuning RandomForest ...\n",
      "  XRP/Close_t+90 => Tuning XGBoost ...\n",
      "    => Best: RF (RMSE=0.768). Saved => ../models\\XRP_Close_t_plus_90_RF.pkl\n",
      "\n",
      "[Info] Saved advanced tuning results => ../data\\advanced_tuning_results.csv\n",
      "[Info] Saved final feature columns => ../models\\final_feature_columns.json\n"
     ]
    }
   ],
   "source": [
    "# 07_advanced_tuning.ipynb (or as a .py script)\n",
    "# ----------------------------------------------------------------------\n",
    "# PURPOSE:\n",
    "#   1) Load {coin}_features.csv for each coin (which you already prepared).\n",
    "#   2) Create multi-horizon targets (Close_t+1, +7, +30, +90).\n",
    "#   3) Time-based split into train/test.\n",
    "#   4) Hyperparam tuning for RandomForest & XGBoost (via RandomizedSearchCV).\n",
    "#   5) Pick whichever model yields lowest RMSE => save as .pkl.\n",
    "#   6) Save overall results to a CSV, plus a JSON listing final feature columns.\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from joblib import dump\n",
    "\n",
    "# ---------------- CONFIG --------------------\n",
    "DATA_FOLDER   = \"../data\"   # where your {coin}_features.csv are\n",
    "MODELS_FOLDER = \"../models\" # folder to save .pkl\n",
    "OUTPUT_CSV    = os.path.join(DATA_FOLDER, \"advanced_tuning_results.csv\")\n",
    "FEATURE_MAP_JSON = os.path.join(MODELS_FOLDER, \"final_feature_columns.json\")\n",
    "\n",
    "# Make sure you adjust this list to match the coins you actually have:\n",
    "ALL_COINS = [\n",
    "    \"ADA\",\"AVAX\",\"BCH\",\"BNB\",\"BTC\",\n",
    "    \"DOGE\",\"DOT\",\"ETH\",\"LEO\",\"LINK\",\n",
    "    \"LTC\",\"MATIC\",\"NEAR\",\"SHIB\",\"SOL\",\n",
    "    \"TON\",\"TRX\",\"UNI\",\"XRP\"\n",
    "]\n",
    "\n",
    "HORIZONS = [1, 7, 30, 90]\n",
    "TRAIN_RATIO = 0.80  # 80% train, 20% test\n",
    "\n",
    "# Number of random-search iterations:\n",
    "N_ITER_RF  = 20\n",
    "N_ITER_XGB = 20\n",
    "\n",
    "# -------------- Helper Functions --------------\n",
    "def create_multi_horizon_targets(df, horizon_list):\n",
    "    \"\"\"\n",
    "    Creates columns like 'Close_t+1','Close_t+7','Close_t+30','Close_t+90'\n",
    "    by shifting the 'Close' column up by each horizon.\n",
    "    \"\"\"\n",
    "    for h in horizon_list:\n",
    "        df[f\"Close_t+{h}\"] = df[\"Close\"].shift(-h)\n",
    "    # Drop rows at the end that no longer have targets:\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def time_based_split(df, ratio=0.8):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame by time index into train/test by ratio.\n",
    "    \"\"\"\n",
    "    cutoff = int(len(df) * ratio)\n",
    "    train_df = df.iloc[:cutoff].copy()\n",
    "    test_df  = df.iloc[cutoff:].copy()\n",
    "    return train_df, test_df\n",
    "\n",
    "def tune_random_forest(X_train, y_train, n_iter=20, random_state=42):\n",
    "    \"\"\"Random-search hyperparam tuning for RandomForest.\"\"\"\n",
    "    rf = RandomForestRegressor(random_state=random_state)\n",
    "    param_distributions = {\n",
    "        \"n_estimators\":      [50, 100, 200, 300, 500],\n",
    "        \"max_depth\":         [3, 5, 7, 10, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\":  [1, 2, 4],\n",
    "    }\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    return search.best_estimator_\n",
    "\n",
    "def tune_xgboost(X_train, y_train, n_iter=20, random_state=42):\n",
    "    \"\"\"Random-search hyperparam tuning for XGBoost.\"\"\"\n",
    "    xgb = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    param_distributions = {\n",
    "        \"n_estimators\":     [50, 100, 200, 300, 500],\n",
    "        \"max_depth\":        [3, 5, 7, 10],\n",
    "        \"learning_rate\":    [0.01, 0.05, 0.1, 0.2],\n",
    "        \"subsample\":        [0.6, 0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    return search.best_estimator_\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(MODELS_FOLDER):\n",
    "        os.makedirs(MODELS_FOLDER)\n",
    "    \n",
    "    all_results = []\n",
    "    feature_map = {}  # will store { (coin, horizon_col) : [feature_cols,...] }\n",
    "\n",
    "    for coin in ALL_COINS:\n",
    "        print(f\"\\n=== Hyperparam Tuning for {coin} ===\")\n",
    "        features_file = os.path.join(DATA_FOLDER, f\"{coin}_features.csv\")\n",
    "        if not os.path.exists(features_file):\n",
    "            print(f\"  [Skipping] CSV not found: {features_file}\")\n",
    "            continue\n",
    "\n",
    "        # 1) Load features\n",
    "        df = pd.read_csv(features_file, parse_dates=[\"Date\"])\n",
    "        df.sort_values(\"Date\", inplace=True)\n",
    "        df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "        # 2) Create multi-horizon targets\n",
    "        df = create_multi_horizon_targets(df, HORIZONS)\n",
    "        if len(df) < 50:\n",
    "            print(f\"  [Skipping] Not enough rows (<50) after shift for {coin}.\")\n",
    "            continue\n",
    "        \n",
    "        # 3) Train/test split\n",
    "        train_df, test_df = time_based_split(df, TRAIN_RATIO)\n",
    "        if len(test_df) < 10:\n",
    "            print(f\"  [Skipping] Not enough test rows (<10) for {coin}.\")\n",
    "            continue\n",
    "        \n",
    "        # Identify which columns are numeric but not horizon columns\n",
    "        horizon_cols = [c for c in df.columns if c.startswith(\"Close_t+\")]\n",
    "        # Exclude any non-numeric or these target columns\n",
    "        exclude_list = set(horizon_cols + [\"Name\",\"Symbol\",\"SNo\",\"High\",\"Low\",\"Open\",\"Volume\",\"Marketcap\"])\n",
    "        feature_cols = [\n",
    "            c for c in df.columns\n",
    "            if (c not in exclude_list) and pd.api.types.is_numeric_dtype(df[c])\n",
    "        ]\n",
    "        \n",
    "        # For each horizon\n",
    "        for horizon_col in horizon_cols:\n",
    "            # Make sure we have enough data not only in train but also in test\n",
    "            tmp_train = train_df.dropna(subset=feature_cols + [horizon_col])\n",
    "            tmp_test  = test_df.dropna(subset=feature_cols + [horizon_col])\n",
    "            if (len(tmp_train) < 40) or (len(tmp_test) < 10):\n",
    "                print(f\"  {coin}/{horizon_col} => Not enough data after dropna. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            X_train = tmp_train[feature_cols]\n",
    "            y_train = tmp_train[horizon_col]\n",
    "            X_test  = tmp_test[feature_cols]\n",
    "            y_test  = tmp_test[horizon_col]\n",
    "            \n",
    "            # 4) Tune RF\n",
    "            print(f\"  {coin}/{horizon_col} => Tuning RandomForest ...\")\n",
    "            best_rf  = tune_random_forest(X_train, y_train, n_iter=N_ITER_RF)\n",
    "            # 5) Tune XGB\n",
    "            print(f\"  {coin}/{horizon_col} => Tuning XGBoost ...\")\n",
    "            best_xgb = tune_xgboost(X_train, y_train, n_iter=N_ITER_XGB)\n",
    "            \n",
    "            # Evaluate\n",
    "            rf_pred  = best_rf.predict(X_test)\n",
    "            xgb_pred = best_xgb.predict(X_test)\n",
    "            \n",
    "            rf_rmse  = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "            rf_mae   = mean_absolute_error(y_test, rf_pred)\n",
    "            xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "            xgb_mae  = mean_absolute_error(y_test, xgb_pred)\n",
    "            \n",
    "            # Decide best\n",
    "            if xgb_rmse < rf_rmse:\n",
    "                best_model = best_xgb\n",
    "                best_model_type = \"XGB\"\n",
    "                best_rmse = xgb_rmse\n",
    "            else:\n",
    "                best_model = best_rf\n",
    "                best_model_type = \"RF\"\n",
    "                best_rmse = rf_rmse\n",
    "            \n",
    "            # Save best model\n",
    "            horizon_sanitized = horizon_col.replace(\"+\",\"_plus_\")\n",
    "            model_filename = f\"{coin}_{horizon_sanitized}_{best_model_type}.pkl\"\n",
    "            model_path     = os.path.join(MODELS_FOLDER, model_filename)\n",
    "            dump(best_model, model_path)\n",
    "            \n",
    "            print(f\"    => Best: {best_model_type} (RMSE={best_rmse:.3f}). Saved => {model_path}\")\n",
    "            \n",
    "            all_results.append({\n",
    "                \"Coin\"      : coin,\n",
    "                \"Horizon\"   : horizon_col,\n",
    "                \"RF_RMSE\"   : rf_rmse,\n",
    "                \"RF_MAE\"    : rf_mae,\n",
    "                \"XGB_RMSE\"  : xgb_rmse,\n",
    "                \"XGB_MAE\"   : xgb_mae,\n",
    "                \"BestModel\" : best_model_type,\n",
    "                \"BestRMSE\"  : best_rmse\n",
    "            })\n",
    "            \n",
    "            # record feature columns for later usage (fetch_and_predict)\n",
    "            feature_map_key = (coin, horizon_col)\n",
    "            feature_map[feature_map_key] = feature_cols\n",
    "\n",
    "    # Wrap up\n",
    "    if len(all_results) == 0:\n",
    "        print(\"\\n[Info] No valid data found for any coins. No models have been saved.\")\n",
    "        return\n",
    "    \n",
    "    # Create a results DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.sort_values([\"Coin\",\"Horizon\"], inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n[Info] Saved advanced tuning results => {OUTPUT_CSV}\")\n",
    "    \n",
    "    # Save feature map as JSON\n",
    "    fm_dict = {}\n",
    "    for (coin,horizon_col), feats in feature_map.items():\n",
    "        fm_dict[f\"{coin}__{horizon_col}\"] = feats\n",
    "    \n",
    "    with open(FEATURE_MAP_JSON, \"w\") as f:\n",
    "        json.dump(fm_dict, f, indent=2)\n",
    "    print(f\"[Info] Saved final feature columns => {FEATURE_MAP_JSON}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
